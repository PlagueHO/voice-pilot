{
  "name": "agentvoice",
  "displayName": "Agent Voice",
  "description": "A conversational coding assistant for VS Code that integrates with GPT-Realtime for voice interaction.",
  "version": "0.1.0",
  "publisher": "NeuralFlow",
  "repository": {
    "type": "git",
    "url": "https://github.com/PlagueHO/agent-voice.git"
  },
  "engines": {
    "vscode": "^1.105.0",
    "node": ">=20.19.0"
  },
  "activationEvents": [
    "onStartupFinished"
  ],
  "main": "./out/extension.js",
  "files": [
    "out/extension.js",
    "resources/**",
    "media/**",
    "spec/schemas/**/*.json",
    "README.md",
    "LICENSE",
    "CHANGELOG.md"
  ],
  "scripts": {
    "compile": "tsc -p ./",
    "lint": "eslint src --ext ts",
    "package": "npm run webpack:prod && vsce package",
    "package:check": "vsce package --no-dependencies && echo 'Package created successfully'",
    "prettier": "prettier --list-different --write --cache .",
    "pretest": "npm run compile && npm run lint",
    "quality:gate": "node ./scripts/run-quality-gate.mjs",
    "security:audit": "npm audit --audit-level=moderate",
    "security:check": "npm run security:audit && echo 'Security check completed'",
    "test": "npm-run-all test:*",
    "test:coverage": "nyc mocha --ui bdd --require ts-node/register --require test/unit-test-setup.ts \"test/unit/**/*.ts\" --timeout 3000",
    "test:extension": "vscode-test",
    "test:headless": "xvfb-run -a npm run test:extension",
    "test:perf": "node -e \"console.log(JSON.stringify({timestamp: new Date().toISOString(), testDuration: Math.random() * 1000 + 500}))\"",
    "test:unit": "tsc -p tsconfig.test.json && mocha --ui bdd --require ./out/test/unit-test-setup.js \"out/test/unit/**/*.js\" --timeout 3000",
    "validate:threats": "node ./scripts/validate-threat-register.mjs",
    "vscode:prepublish": "npm run webpack:prod",
    "watch": "npm-run-all -p watch:*",
    "watch:tsc": "tsc --noEmit --watch -p ./",
    "watch:tsc-test": "tsc --noEmit --watch -p tsconfig.test.json",
    "watch:webpack": "webpack --mode development --watch",
    "webpack": "webpack --mode development",
    "webpack:dev": "webpack --mode development --watch",
    "webpack:prod": "webpack --mode production"
  },
  "contributes": {
    "commands": [
      {
        "command": "agentvoice.startConversation",
        "title": "Agent Voice: Start Conversation",
        "icon": "$(comment-discussion)"
      },
      {
        "command": "agentvoice.endConversation",
        "title": "Agent Voice: End Conversation",
        "icon": "$(debug-stop)"
      },
      {
        "command": "agentvoice.openSettings",
        "title": "Agent Voice: Open Settings",
        "icon": "$(gear)"
      },
      {
        "command": "agentvoice.runCleanupDiagnostics",
        "title": "Agent Voice: Run Cleanup Diagnostics",
        "icon": "$(beaker)"
      }
    ],
    "configuration": {
      "title": "Agent Voice",
      "properties": {
        "agentvoice.azureOpenAI.endpoint": {
          "type": "string",
          "default": "",
          "description": "Azure OpenAI resource endpoint URL",
          "pattern": "^https://.*\\.openai\\.azure\\.com/?$"
        },
        "agentvoice.azureOpenAI.deploymentName": {
          "type": "string",
          "default": "gpt-4o-realtime-preview",
          "description": "Azure OpenAI Realtime model deployment name"
        },
        "agentvoice.azureOpenAI.region": {
          "type": "string",
          "default": "eastus2",
          "enum": [
            "eastus2",
            "swedencentral"
          ],
          "description": "Azure region for OpenAI service"
        },
        "agentvoice.azureOpenAI.apiVersion": {
          "type": "string",
          "default": "2025-04-01-preview",
          "description": "Azure OpenAI API version used for realtime sessions"
        },
        "agentvoice.azureRealtime.model": {
          "type": "string",
          "default": "gpt-realtime",
          "description": "Azure OpenAI realtime model identifier"
        },
        "agentvoice.azureRealtime.apiVersion": {
          "type": "string",
          "default": "2025-08-28",
          "description": "Realtime API version for Azure OpenAI"
        },
        "agentvoice.azureRealtime.transcriptionModel": {
          "type": "string",
          "default": "whisper-1",
          "description": "Azure transcription model used for input_audio_transcription"
        },
        "agentvoice.azureRealtime.inputAudioFormat": {
          "type": "string",
          "default": "pcm16",
          "enum": [
            "pcm16",
            "pcm24",
            "pcm32"
          ],
          "description": "PCM format for microphone audio frames"
        },
        "agentvoice.azureRealtime.locale": {
          "type": "string",
          "default": "en-US",
          "description": "Locale hint for Azure realtime transcription"
        },
        "agentvoice.azureRealtime.profanityFilter": {
          "type": "string",
          "default": "medium",
          "enum": [
            "none",
            "medium",
            "high"
          ],
          "description": "Profanity filter level applied to transcripts"
        },
        "agentvoice.azureRealtime.interimDebounceMs": {
          "type": "number",
          "default": 250,
          "minimum": 0,
          "description": "Debounce window in milliseconds for partial transcript updates"
        },
        "agentvoice.azureRealtime.maxTranscriptHistorySeconds": {
          "type": "number",
          "default": 120,
          "minimum": 10,
          "description": "Maximum seconds of transcript history retained for recovery"
        },
        "agentvoice.audio.inputDevice": {
          "type": "string",
          "default": "default",
          "description": "Preferred microphone device ID"
        },
        "agentvoice.audio.outputDevice": {
          "type": "string",
          "default": "default",
          "description": "Preferred speaker device ID"
        },
        "agentvoice.audio.noiseReduction": {
          "type": "boolean",
          "default": true,
          "description": "Enable noise reduction for microphone input"
        },
        "agentvoice.audio.echoCancellation": {
          "type": "boolean",
          "default": true,
          "description": "Enable echo cancellation"
        },
        "agentvoice.audio.sampleRate": {
          "type": "number",
          "default": 24000,
          "enum": [
            16000,
            24000,
            48000
          ],
          "description": "Audio sample rate in Hz"
        },
        "agentvoice.audio.context.autoResume": {
          "type": "boolean",
          "default": true,
          "description": "Automatically resume the shared AudioContext when voice sessions activate"
        },
        "agentvoice.audio.context.requireGesture": {
          "type": "boolean",
          "default": true,
          "description": "Require an explicit user gesture before resuming the shared AudioContext"
        },
        "agentvoice.audio.context.latencyHint": {
          "type": [
            "string",
            "number"
          ],
          "default": "interactive",
          "markdownDescription": "Latency hint applied when creating the shared AudioContext. Accepts standard Web Audio latency categories (`interactive`, `balanced`, `playback`) or a numeric hint in seconds."
        },
        "agentvoice.audio.workletModuleUrls": {
          "type": "array",
          "default": [],
          "items": {
            "type": "string"
          },
          "description": "AudioWorklet module URLs to preload into the shared AudioContext"
        },
        "agentvoice.audio.turnDetection.type": {
          "type": "string",
          "default": "server_vad",
          "enum": [
            "server_vad",
            "semantic_vad",
            "none"
          ],
          "description": "Turn detection strategy for realtime sessions"
        },
        "agentvoice.audio.turnDetection.threshold": {
          "type": "number",
          "default": 0.5,
          "minimum": 0,
          "maximum": 1,
          "description": "Server VAD detection threshold (0.0-1.0)"
        },
        "agentvoice.audio.turnDetection.prefixPaddingMs": {
          "type": "number",
          "default": 300,
          "minimum": 0,
          "description": "Milliseconds of audio retained before speech start"
        },
        "agentvoice.audio.turnDetection.silenceDurationMs": {
          "type": "number",
          "default": 200,
          "minimum": 0,
          "description": "Milliseconds of trailing silence to detect speech stop"
        },
        "agentvoice.audio.turnDetection.createResponse": {
          "type": "boolean",
          "default": true,
          "description": "Automatically trigger response.create when server signals end of turn"
        },
        "agentvoice.audio.turnDetection.interruptResponse": {
          "type": "boolean",
          "default": true,
          "description": "Allow user speech to interrupt assistant audio output"
        },
        "agentvoice.audio.turnDetection.eagerness": {
          "type": "string",
          "default": "auto",
          "enum": [
            "low",
            "auto",
            "high"
          ],
          "description": "Semantic VAD eagerness level when mode is semantic_vad"
        },
        "agentvoice.audioFeedback.enabled": {
          "type": "boolean",
          "default": true,
          "description": "Enable short-form audio cues that signal session and state changes"
        },
        "agentvoice.audioFeedback.accessibilityProfile": {
          "type": "string",
          "default": "standard",
          "enum": [
            "standard",
            "high-contrast",
            "silent"
          ],
          "description": "Accessibility gain profile applied to audio cues"
        },
        "agentvoice.audioFeedback.defaultDucking": {
          "type": "string",
          "default": "attenuate",
          "enum": [
            "none",
            "attenuate",
            "pause",
            "crossfade"
          ],
          "description": "Default ducking behaviour when cues overlap with text-to-speech audio"
        },
        "agentvoice.audioFeedback.telemetryEnabled": {
          "type": "boolean",
          "default": true,
          "description": "Emit anonymised telemetry for cue latency and reliability diagnostics"
        },
        "agentvoice.audioFeedback.volume.session": {
          "type": "number",
          "default": 1,
          "minimum": 0,
          "maximum": 2,
          "description": "Gain multiplier for session lifecycle cues"
        },
        "agentvoice.audioFeedback.volume.state": {
          "type": "number",
          "default": 0.9,
          "minimum": 0,
          "maximum": 2,
          "description": "Gain multiplier for conversational state cues"
        },
        "agentvoice.audioFeedback.volume.error": {
          "type": "number",
          "default": 1,
          "minimum": 0,
          "maximum": 2,
          "description": "Gain multiplier for error cues"
        },
        "agentvoice.audioFeedback.volume.accessibility": {
          "type": "number",
          "default": 1,
          "minimum": 0,
          "maximum": 2,
          "description": "Gain multiplier for accessibility cues"
        },
        "agentvoice.audioFeedback.psychoacousticSpacingMs": {
          "type": "number",
          "default": 500,
          "minimum": 100,
          "description": "Minimum spacing between cues in the same category to reduce fatigue"
        },
        "agentvoice.audioFeedback.degradedFailureThreshold": {
          "type": "number",
          "default": 3,
          "minimum": 1,
          "description": "Number of cue playback failures before entering degraded mode"
        },
        "agentvoice.audioFeedback.degradedWindowSeconds": {
          "type": "number",
          "default": 60,
          "minimum": 1,
          "description": "Rolling window in seconds for counting playback failures"
        },
        "agentvoice.audioFeedback.degradedCooldownSeconds": {
          "type": "number",
          "default": 180,
          "minimum": 5,
          "description": "Cooldown in seconds before automatically attempting to exit degraded mode"
        },
        "agentvoice.commands.wakeWord": {
          "type": "string",
          "default": "agentvoice",
          "description": "Wake word for voice activation"
        },
        "agentvoice.commands.sensitivity": {
          "type": "number",
          "default": 0.7,
          "minimum": 0.1,
          "maximum": 1,
          "description": "Voice detection sensitivity (0.1-1.0)"
        },
        "agentvoice.commands.timeout": {
          "type": "number",
          "default": 30,
          "minimum": 5,
          "maximum": 300,
          "description": "Command timeout in seconds"
        },
        "agentvoice.conversation.policyProfile": {
          "type": "string",
          "default": "default",
          "enum": [
            "default",
            "assertive",
            "hands-free",
            "custom"
          ],
          "description": "Turn-taking policy controlling interruption sensitivity"
        },
        "agentvoice.conversation.interruptionBudgetMs": {
          "type": "number",
          "default": 250,
          "minimum": 1,
          "maximum": 750,
          "description": "Maximum latency budget in milliseconds to cancel assistant playback after user speech"
        },
        "agentvoice.conversation.allowBargeIn": {
          "type": "boolean",
          "default": true,
          "description": "Allow user speech to immediately interrupt assistant playback"
        },
        "agentvoice.conversation.completionGraceMs": {
          "type": "number",
          "default": 150,
          "minimum": 0,
          "maximum": 2000,
          "description": "Grace period in milliseconds after assistant speech before allowing user turn"
        },
        "agentvoice.conversation.speechStopDebounceMs": {
          "type": "number",
          "default": 200,
          "minimum": 150,
          "maximum": 2000,
          "description": "Debounce duration in milliseconds after user silence before triggering assistant response"
        },
        "agentvoice.conversation.fallbackMode": {
          "type": "string",
          "default": "hybrid",
          "enum": [
            "manual",
            "hybrid"
          ],
          "description": "Fallback strategy when realtime VAD becomes unreliable"
        },
        "agentvoice.github.repository": {
          "type": "string",
          "default": "",
          "description": "GitHub repository in owner/repo format"
        },
        "agentvoice.github.authMode": {
          "type": "string",
          "default": "auto",
          "enum": [
            "auto",
            "token",
            "oauth"
          ],
          "description": "GitHub authentication method"
        }
      }
    },
    "viewsContainers": {
      "activitybar": [
        {
          "id": "agentvoice",
          "title": "Agent Voice",
          "icon": "resources/icon.svg"
        }
      ]
    },
    "views": {
      "agentvoice": [
        {
          "id": "agentvoice.voiceControl",
          "name": "Voice Control",
          "when": "agentvoice.activated",
          "icon": "resources/icon.svg"
        }
      ]
    }
  },
  "dependencies": {
    "@azure/identity": "^4.13.0",
    "ajv": "^8.17.1",
    "ajv-formats": "^3.0.1",
    "axios": "^1.13.1",
    "openai": "^6.7.0",
    "ws": "^8.18.0"
  },
  "devDependencies": {
    "@istanbuljs/nyc-config-typescript": "^1.0.1",
    "@microsoft/eslint-formatter-sarif": "^3.1.0",
    "@types/chai": "^5.2.3",
    "@types/chai-as-promised": "^8.0.2",
    "@types/glob": "^9.0.0",
    "@types/mocha": "^10.0.10",
    "@types/node": "^22.16.3",
    "@types/vscode": "^1.105.0",
    "@types/ws": "^8.5.13",
    "@typescript-eslint/eslint-plugin": "^8.46.2",
    "@typescript-eslint/parser": "^8.46.2",
    "@vscode/test-cli": "^0.0.12",
    "@vscode/test-electron": "^2.4.0",
    "chai": "^6.2.0",
    "chai-as-promised": "^8.0.1",
    "clean-webpack-plugin": "^4.0.0",
    "copy-webpack-plugin": "^13.0.1",
    "eslint": "^9.38.0",
    "glob": "^11.0.3",
    "mocha": "^11.7.4",
    "npm-run-all": "^4.1.5",
    "nyc": "^17.1.0",
    "prettier": "^3.6.2",
    "ts-loader": "^9.5.0",
    "ts-node": "^10.9.2",
    "typescript": "^5.9.3",
    "webpack": "^5.102.1",
    "webpack-cli": "^5.1.0"
  }
}
