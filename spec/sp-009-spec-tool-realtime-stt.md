---
title: Realtime Speech-to-Text Integration
version: 1.0
date_created: 2025-09-25
last_updated: 2025-09-27
owner: Agent Voice Project
tags: [tool, speech-to-text, realtime, azure-openai, transcription]
---

# Introduction

This specification defines the realtime speech-to-text (STT) integration that converts live microphone audio captured in the Agent Voice webview into structured transcription events using Azure OpenAI GPT Realtime API. The STT layer bridges the audio capture pipeline, WebRTC transport, and conversational UX so that users receive low-latency transcripts, intent extraction inputs, and UI feedback aligned with Agent Voice's accessibility-first design.

## 1. Purpose & Scope

This specification covers the functional, architectural, and operational requirements for realtime speech-to-text within Agent Voice:

- Transform PCM16 audio streams delivered over the WebRTC transport (SP-006) into incremental transcription results.
- Coordinate closely with the audio capture pipeline (SP-007) and session management (SP-005) to ensure authenticated, resilient streaming.
- Provide structured transcript events for downstream consumers (intent processor, UI panel, session logs) while respecting privacy and performance budgets.
- Handle interruption, turn-taking, and VAD cues to keep the conversation natural per UI guidelines (UI.md).
- Ensure secure, reliable operation with Azure OpenAI realtime modalities, including fallback behaviors and error diagnostics.

**Intended Audience**: Extension developers integrating conversational flows, audio/WebRTC engineers, and Azure OpenAI specialists.

**Assumptions**:

- WebRTC transport is already established with duplex audio streams (SP-006).
- Audio capture pipeline produces PCM16 mono audio at 24 kHz with noise reduction enabled (SP-007).
- The webview reuses the shared Web Audio API 1.1 `AudioContext` (default render quantum size 128 frames) provided by the capture pipeline; the STT layer MUST NOT create additional contexts.
- Session Manager (SP-005) handles ephemeral key renewals and session lifecycle events.
- UI components consume transcript deltas and status updates for display and accessibility feedback (UI.md, COMPONENTS.md).
- Voice Activity Detection (SP-008) will supply turn-taking hints but is not yet available; interim logic relies on Azure server VAD.

## 2. Definitions

- **Azure Realtime Transcription**: Incremental text output generated by Azure OpenAI GPT Realtime API from streamed audio.
- **Transcript Delta**: Partial transcription fragment representing new text since the last emitted result.
- **Transcript Aggregator**: Component that assembles deltas into coherent utterances with punctuation and metadata.
- **Server VAD**: Azure-hosted voice activity detection signals included in realtime events for turn-taking.
- **Client VAD**: Local voice activity detection results from the audio pipeline (future dependency).
- **In-Flight Utterance**: Active transcription segment that may still receive additional deltas.
- **Finalized Utterance**: Completed transcription segment marked as stable and safe to persist.
- **Turn Boundary**: Transition point between user speech and assistant speech.
- **Redaction Policy**: Rules governing masking/removal of sensitive content before persistence or display.
- **Transcript Cache**: Temporary storage of recent transcription events for re-synchronization after transport hiccups.

## 3. Requirements, Constraints & Guidelines

### Functional Requirements

- **REQ-001**: STT service SHALL stream PCM16 audio frames to Azure OpenAI GPT Realtime API via the established WebRTC session.
- **REQ-002**: STT service SHALL emit transcript delta events within 250 ms of receipt from Azure.
- **REQ-003**: STT service SHALL provide utterance lifecycle events (`partial`, `final`) with unique identifiers.
- **REQ-004**: STT service SHALL synchronize with Session Manager to pause or resume transcription during key renewal.
- **REQ-005**: STT service SHALL support multi-modal sessions (`['audio','text']`) configured by WebRTC session update events.
- **REQ-006**: STT service SHALL expose transcription confidence scores when available from Azure metadata.
- **REQ-007**: STT service SHALL issue a `session.update` command selecting the realtime model and enabling `input_audio_transcription` before streaming audio, as described in the Azure GPT Realtime quickstart.
- **REQ-008**: STT service SHALL trigger `response.create` after configuring the session so that Azure begins emitting transcript deltas.
- **REQ-009**: STT service SHALL configure `input_audio_transcription.model` (default `whisper-1`), `input_audio_format`, and `turn_detection` properties in `session.update` requests so that Azure emits transcription events and handles VAD per session settings.
- **REQ-010**: STT service SHALL ingest `session.updated`, `input_audio_buffer.speech_*`, and `conversation.item.audio_transcription.*` events to keep local state synchronized with Azure Realtime API behavior.
- **REQ-011**: STT service SHALL reuse the shared Web Audio API 1.1 `AudioContext`, validating `audioContext.renderQuantumSize` matches the negotiated frame size (default 128) before streaming audio.

### Speech-to-Text Specific Requirements

- **STT-001**: Transcript aggregation SHALL preserve punctuation and casing supplied by Azure deltas.
- **STT-002**: Transcript aggregator SHALL debounce partial updates to avoid UI flicker while keeping latency below 500 ms.
- **STT-003**: Transcript events SHALL include timestamps relative to session start and absolute UTC time.
- **STT-004**: STT service SHALL detect Azure `response.output_audio_transcript.delta` events and map them to transcript deltas.
- **STT-005**: STT service SHALL acknowledge `response.done` events to finalize outstanding utterances.
- **STT-006**: STT service SHALL support optional profanity filtering and custom redaction rules (configurable).
- **STT-007**: Transcript buffer SHALL retain at least the last 120 seconds of text for reconnection recovery.
- **STT-008**: STT service SHALL tag transcripts with speaker role (`user`, `assistant`, `system`) as inferred from server events and UI state.
- **STT-009**: STT service SHALL support selecting Azure realtime model identifiers (for example, `gpt-realtime`, `gpt-4o-realtime-preview`) and API versions (for example, `2025-08-28`) supplied by configuration.
- **STT-010**: STT service SHALL expose configuration for `turn_detection` modes (`none`, `server_vad`, `semantic_vad`) including `create_response`, thresholds, padding, and silence duration parameters.
- **STT-011**: STT service SHALL surface `input_audio_buffer.speech_started`, `speech_stopped`, and `committed` signals to downstream consumers (UI, interruption manager) within 200 ms of receipt.
- **STT-012**: STT service SHALL route capture frames through `AudioWorkletNode` processors (Web Audio API 1.1) instead of deprecated `ScriptProcessorNode`, ensuring custom processing executes on the audio rendering thread.

### Security Requirements

- **SEC-001**: STT service SHALL never log raw audio frames or unredacted transcripts to persistent storage.
- **SEC-002**: Transcript events transmitted to UI SHALL respect privacy/redaction policies defined by future SP-027.
- **SEC-003**: STT service SHALL validate that WebRTC data channel messages originate from trusted Azure endpoints.
- **SEC-004**: Credential handoff to the webview SHALL use ephemeral keys supplied by Session Manager; long-lived keys are forbidden.

### Constraints

- **CON-001**: Average end-to-end transcription latency SHALL be ≤ 1.5 seconds under normal network conditions.
- **CON-002**: STT service SHALL operate within browser memory constraints (≤ 25 MB dedicated buffer/caches).
- **CON-003**: STT service SHALL degrade gracefully on packet loss up to 5% without crashing; missing text must be flagged.
- **CON-004**: Transcript history exposed to UI SHALL cap at 500 utterances per session to maintain performance.

### Guidelines

- **GUD-001**: Prefer streaming APIs over REST callbacks to minimize latency.
- **GUD-002**: Use structured logging with correlation IDs matching Session Manager and WebRTC connection IDs.
- **GUD-003**: Provide localized status strings for UI states (`Listening`, `Thinking`, `Transcribing`) aligning with UI.md accessibility guidance.
- **GUD-004**: Implement pluggable post-processing (e.g., capitalization, filler word removal) without tight coupling to Azure responses.
- **GUD-005**: Stage AudioWorklet modules during initialization and share them through the singleton `AudioContext` so render quanta remain synchronized and duplicate module loads are avoided.

### Patterns

- **PAT-001**: Apply Observer pattern for transcript event subscriptions (UI, intent processor, telemetry).
- **PAT-002**: Use State Machine pattern for utterance lifecycle (`Pending → Partial → Finalized → Archived`).
- **PAT-003**: Use Strategy pattern to switch between server-side VAD and future client-side VAD providers.
- **PAT-004**: Apply Circuit Breaker pattern around Azure transcript streams to prevent cascading failures.

## 4. Interfaces & Data Contracts

### SpeechToTextService Interface

```typescript
import { ServiceInitializable } from '../core/service-initializable';
import { SessionInfo } from '../session/session-manager';

export interface SpeechToTextService extends ServiceInitializable {
  startTranscription(session: SessionInfo, options?: TranscriptionOptions): Promise<void>;
  stopTranscription(sessionId: string): Promise<void>;
  pauseTranscription(sessionId: string, reason: PauseReason): Promise<void>;
  resumeTranscription(sessionId: string): Promise<void>;

  getActiveUtterances(sessionId: string): UtteranceSnapshot[];
  getTranscriptHistory(sessionId: string, limit?: number): TranscriptEntry[];
  clearTranscriptHistory(sessionId: string): Promise<void>;

  onTranscriptEvent(handler: TranscriptEventHandler): Disposable;
  onStatusEvent(handler: TranscriptionStatusHandler): Disposable;
  onError(handler: TranscriptionErrorHandler): Disposable;
}

export interface TranscriptionOptions {
  profanityFilter?: 'none' | 'medium' | 'high';
  redactionRules?: RedactionRule[];
  speakerHint?: SpeakerRole;
  locale?: string; // e.g., 'en-US'
  interimDebounceMs?: number; // default 250 ms
  model?: string; // Azure realtime deployment name (e.g., 'gpt-realtime')
  apiVersion?: string; // Azure API version (e.g., '2025-08-28')
  transcriptionModel?: string; // Azure transcription model (e.g., 'whisper-1')
  turnDetection?: TurnDetectionConfig;
  audioContext?: BaseAudioContext; // Shared Web Audio API 1.1 context provided by capture pipeline
  expectedRenderQuantumSize?: number; // Defaults to 128 frames per Web Audio API 1.1
}

export interface UtteranceSnapshot {
  utteranceId: string;
  sessionId: string;
  speaker: SpeakerRole;
  content: string;
  confidence?: number;
  createdAt: string; // ISO timestamp
  updatedAt: string; // ISO timestamp
  status: UtteranceStatus;
  metadata: UtteranceMetadata;
}

export interface TranscriptEntry extends UtteranceSnapshot {
  final: boolean;
  sequence: number;
}

export type SpeakerRole = 'user' | 'assistant' | 'system';

export type UtteranceStatus = 'pending' | 'partial' | 'final' | 'archived';

export interface UtteranceMetadata {
  startOffsetMs: number;
  endOffsetMs?: number;
  locale: string;
  serverVad?: ServerVadSignal;
  clientVad?: ClientVadSignal;
  redactionsApplied?: RedactionMatch[];
  chunkCount: number;
}

export interface TurnDetectionConfig {
  type: 'none' | 'server_vad' | 'semantic_vad';
  threshold?: number;
  prefixPaddingMs?: number;
  silenceDurationMs?: number;
  createResponse?: boolean;
}
```

### Transcript Events

```typescript
export type TranscriptEventHandler = (event: TranscriptEvent) => void | Promise<void>;
export type TranscriptionStatusHandler = (event: TranscriptionStatusEvent) => void | Promise<void>;
export type TranscriptionErrorHandler = (event: TranscriptionErrorEvent) => void | Promise<void>;

export type TranscriptEvent =
  | TranscriptDeltaEvent
  | TranscriptFinalEvent
  | TranscriptRedoEvent
  | TranscriptClearedEvent;

export interface TranscriptDeltaEvent {
  type: 'transcript-delta';
  sessionId: string;
  utteranceId: string;
  delta: string;
  content: string;
  confidence?: number;
  timestamp: string;
  sequence: number;
  metadata: UtteranceMetadata;
}

export interface TranscriptFinalEvent {
  type: 'transcript-final';
  sessionId: string;
  utteranceId: string;
  content: string;
  confidence?: number;
  timestamp: string;
  metadata: UtteranceMetadata;
}

export interface TranscriptRedoEvent {
  type: 'transcript-redo';
  sessionId: string;
  utteranceId: string;
  previousContent: string;
  replacementContent: string;
  reason: 'desync' | 'confidence-drop' | 'redaction';
  timestamp: string;
}

export interface TranscriptClearedEvent {
  type: 'transcript-cleared';
  sessionId: string;
  clearedAt: string;
  reason: 'user-requested' | 'privacy-policy' | 'session-end';
}

export interface TranscriptionStatusEvent {
  type: 'transcription-status';
  sessionId: string;
  status: 'connecting' | 'listening' | 'thinking' | 'paused' | 'error' | 'speech-started' | 'speech-stopped';
  detail?: string;
  timestamp: string;
}

export interface TranscriptionErrorEvent {
  type: 'transcription-error';
  sessionId: string;
  code: TranscriptionErrorCode;
  message: string;
  recoverable: boolean;
  remediation?: string;
  timestamp: string;
  context?: any;
}

export enum TranscriptionErrorCode {
  TransportDisconnected = 'TRANSPORT_DISCONNECTED',
  AuthenticationFailed = 'AUTHENTICATION_FAILED',
  AudioStreamStalled = 'AUDIO_STREAM_STALLED',
  ResponseFormatInvalid = 'RESPONSE_FORMAT_INVALID',
  RateLimited = 'RATE_LIMITED',
  ProfanityFilterFailed = 'PROFANITY_FILTER_FAILED',
  RedactionRuleInvalid = 'REDACTION_RULE_INVALID',
  Unknown = 'UNKNOWN'
}
```

### Azure Realtime Mapping

```typescript
export interface AzureRealtimeTranscriptMessage {
  type: 'response.output_audio_transcript.delta' | 'response.done';
  response_id: string;
  item_id: string;
  delta?: {
    text?: string;
    confidence?: number;
    annotations?: AzureAnnotation[];
  };
  final?: {
    text: string;
    confidence?: number;
  };
  server_vad?: ServerVadSignal;
  timestamp: string;
}

export interface ServerVadSignal {
  state: 'start' | 'stop';
  offset_ms: number;
}

export interface ClientVadSignal {
  state: 'start' | 'stop';
  confidence: number;
  offset_ms: number;
}

export interface RedactionRule {
  id: string;
  pattern: RegExp | string;
  replacement: string;
  explanation?: string;
}

export interface RedactionMatch {
  ruleId: string;
  originalText: string;
  replacementText: string;
  startIndex: number;
  endIndex: number;
}
```

## 5. Acceptance Criteria

- **AC-001**: Given an active session with valid credentials, When `startTranscription()` is invoked, Then transcript delta events begin within 1 second of user speech.
- **AC-002**: Given ongoing transcription, When Azure emits `response.done`, Then the corresponding utterance is finalized and marked immutable.
- **AC-003**: Given network interruption under 5 seconds, When transport reconnects, Then transcript history is replayed without duplicate final utterances.
- **AC-004**: Given profanity filtering enabled, When blocked terms appear, Then redacted content is produced and metadata records the applied rule.
- **AC-005**: Given session renewal, When Session Manager pauses audio, Then STT service pauses without losing buffered audio and resumes automatically.
- **AC-006**: Given UI consumption, When transcripts update rapidly, Then debounced events keep UI state synchronized without exceeding 4 updates per second per utterance.
- **AC-007**: Given authentication failure, When Azure rejects the stream, Then STT service surfaces `AUTHENTICATION_FAILED` with remediation guidance and stops gracefully.
- **AC-008**: Given accessibility settings, When status changes occur, Then `TranscriptionStatusEvent` provides localized strings for Voice Control Panel display.
- **AC-009**: Given large utterances (>30 seconds), When finalization occurs, Then transcript chunking preserves order and timestamps without data loss.
- **AC-010**: Given redaction rules cleared, When transcripts are exported to intent processor, Then no residual redaction metadata leaks to UI.
- **AC-011**: Given `server_vad` turn detection, When Azure emits `input_audio_buffer.speech_started` and `speech_stopped`, Then the STT service forwards status updates within 200 ms and schedules `response.create` if `create_response` is false.
- **AC-012**: Given Web Audio 1.1 capture integration, When transcription starts, Then telemetry confirms a single shared `AudioContext` with render quantum size 128 and no additional contexts are created by the STT layer.

## 6. Test Automation Strategy

- **Test Levels**: Unit tests for transcript aggregation, integration tests with mocked Azure realtime messages, end-to-end tests using emulator scripts pushing audio samples via WebRTC.
- **Frameworks**: Mocha + Sinon for unit and integration, Playwright for webview E2E, custom Node harness generating Azure-compatible realtime events.
- **Test Data Management**: Curated audio fixtures (noise, accents, silence), JSON fixtures of Azure realtime responses, profanity/redaction rule sets.
- **CI/CD Integration**: Automated tests triggered via `Test Unit` (mocked) and optional `Test Extension` scenario using prerecorded audio. Use feature flag to avoid live Azure dependency in CI.
- **Coverage Requirements**: ≥95% statement coverage for transcript aggregation, ≥90% branch coverage on error handling paths.
- **Performance Testing**: Measure latency using synthetic timestamps, stress test with bursty audio frames, benchmark reconnection replay.
- **Web Audio Compliance Testing**: Assert that `AudioWorklet` processors run with 128-frame render quanta and fail builds if additional `AudioContext` instances or `ScriptProcessorNode` usage is detected.
- **Localization Testing**: Validate status strings and transcripts for supported locales (initially `en-US`, `en-GB`).
- **Security Testing**: Ensure redaction and profanity filters apply before persistence or UI emission; verify no raw audio buffers leak via logs.
- **VAD Testing**: Simulate `server_vad` and `semantic_vad` event sequences to verify speech start/stop propagation and manual `response.create` dispatch when required.

## 7. Rationale & Context

Realtime transcription is central to the Agent Voice experience because it drives intent extraction, conversational feedback, and accessibility. The design prioritizes:

1. **Low Latency**: Streaming Azure realtime events avoids the latency of batch STT and keeps conversation flow natural.
2. **Resilience**: Integration with Session Manager and WebRTC transport allows seamless recovery from key renewals or network hiccups.
3. **Privacy & Accessibility**: Redaction hooks and status events align with privacy roadmap (SP-027) and UI accessibility requirements (UI.md).
4. **Extensibility**: Clear contracts allow future enhancements such as client VAD (SP-008), multilingual support, and downstream analytics without refactoring core logic.
5. **Observability**: Structured events and correlation IDs support diagnosing audio or transcription anomalies while respecting security constraints.

## 8. Dependencies & External Integrations

### External Systems

- **EXT-001**: Azure OpenAI GPT Realtime API – Provides audio-to-text transcription over WebRTC.
- **EXT-002**: Azure Identity / Ephemeral Key Endpoint – Supplies short-lived tokens for realtime sessions.

### Third-Party Services

- **SVC-001**: Azure Content Filtering (optional) – Provides profanity filtering when enabled.

### Infrastructure Dependencies

- **INF-001**: WebRTC ICE/STUN/TURN infrastructure defined in SP-006 – Required for maintaining transport connectivity.

### Data Dependencies

- **DAT-001**: Transcript cache stored in webview memory – Retains the last 120 seconds of text for replay; not persisted to disk.

### Technology Platform Dependencies

- **PLT-001**: VS Code Webview (Chromium-based) – Provides WebRTC, Web Audio APIs, and messaging channel to extension host.
- **PLT-002**: Node.js extension host – Receives transcript events for Copilot integration and logging.
- **PLT-003**: Web Audio API 1.1 runtime (Chromium) – Supplies `AudioContext`, `AudioWorklet`, and `renderQuantumSize` contracts consumed by the capture pipeline and STT layer.

### Compliance Dependencies

- **COM-001**: Future Privacy & Data Handling Policy (SP-027) – Will dictate retention and masking rules.

### Internal Specification Dependencies

- **SP-005**: Session Management & Renewal – Supplies session lifecycle, key rotation coordination, and timers.
- **SP-006**: WebRTC Audio Transport Layer – Handles audio stream transport and data channel messaging.
- **SP-007**: Audio Capture Pipeline Architecture – Provides microphone audio streams and VAD hooks.
- **UI.md / COMPONENTS.md**: Drive UI status states, accessibility messaging, and panel behavior.

## 9. Examples & Edge Cases

### Example: Configuring Session for Transcription and VAD

```typescript
await realtimeClient.send({
  type: 'session.update',
  session: {
    model: options.model ?? 'gpt-realtime',
    input_audio_format: 'pcm16',
    input_audio_transcription: {
      model: options.transcriptionModel ?? 'whisper-1'
    },
    turn_detection: {
      type: options.turnDetection?.type ?? 'server_vad',
      threshold: options.turnDetection?.threshold ?? 0.5,
      prefix_padding_ms: options.turnDetection?.prefixPaddingMs ?? 300,
      silence_duration_ms: options.turnDetection?.silenceDurationMs ?? 200,
      create_response: options.turnDetection?.createResponse ?? false
    },
    modalities: ['audio', 'text']
  }
});

if (options.turnDetection?.createResponse === false) {
  await realtimeClient.send({ type: 'response.create' });
}
```

### Example: Handling Azure Transcript Delta

```typescript
async function handleRealtimeMessage(message: AzureRealtimeTranscriptMessage) {
  switch (message.type) {
    case 'response.output_audio_transcript.delta': {
      const utteranceId = transcriptIndex.resolveUtterance(message.response_id, message.item_id);
      const processed = postProcessor.applyRedactions(message.delta?.text ?? '', options.redactionRules);

      aggregator.applyDelta({
        sessionId,
        utteranceId,
        delta: processed.content,
        confidence: message.delta?.confidence,
        serverVad: message.server_vad,
        timestamp: message.timestamp
      });

      emitTranscriptDelta(utteranceId, processed);
      break;
    }
    case 'response.done': {
      aggregator.finalizeUtterance({
        sessionId,
        responseId: message.response_id,
        itemId: message.item_id,
        timestamp: message.timestamp
      });
      break;
    }
    default:
      logger.debug('Ignoring unsupported realtime message type', { type: message.type });
  }
}
```

### Example: Debounced Transcript Update to UI

```typescript
const debouncedEmit = debounce((event: TranscriptDeltaEvent) => {
  vscodeApi.postMessage({
    type: 'agentvoice.transcriptDelta',
    payload: event
  });
}, options.interimDebounceMs ?? 250, { maxWait: 500 });

aggregator.on('delta', (event) => {
  debouncedEmit(event);
  statusBus.emit({ type: 'transcription-status', status: 'listening', sessionId, timestamp: new Date().toISOString() });
});
```

### Example: Sharing AudioWorklet Capture Frames

```typescript
const audioContext = options.audioContext ?? audioCapturePipeline.getAudioContext();
const expectedQuantum = options.expectedRenderQuantumSize ?? 128;

if (audioContext.renderQuantumSize !== expectedQuantum) {
  telemetry.warn('Render quantum mismatch', {
    expected: expectedQuantum,
    actual: audioContext.renderQuantumSize,
    sessionId
  });
}

await audioContext.audioWorklet.addModule(context.asWebviewUri(workletModule).toString());
const captureNode = new AudioWorkletNode(audioContext, 'agentvoice-capture-router', {
  numberOfInputs: 1,
  numberOfOutputs: 1,
  outputChannelCount: [1],
  processorOptions: { sessionId }
});

audioCapturePipeline.connectToWorklet(captureNode);
captureNode.port.onmessage = ({ data }) => {
  if (data.type === 'render-quantum') {
    webRtcChannel.send(data.frame); // 128-frame quanta aligned with Web Audio API 1.1
  }
};
```

### Edge Case: Packet Loss & Re-synchronization

```typescript
async function handleTransportDesync(sessionId: string) {
  logger.warn('Transport desync detected, requesting transcript replay', { sessionId });

  // Signal to Azure for transcript replay if supported
  dataChannel.send(JSON.stringify({ type: 'transcript.replay.request' }));

  // Rehydrate UI from cache to avoid blank states
  const history = sttService.getTranscriptHistory(sessionId, 20);
  for (const entry of history) {
    vscodeApi.postMessage({
      type: 'agentvoice.transcriptHydration',
      payload: entry
    });
  }
}
```

### Edge Case: Profanity Filtering Failure

```typescript
function applyRedactions(text: string, rules: RedactionRule[]): RedactionResult {
  const matches: RedactionMatch[] = [];
  let sanitized = text;

  for (const rule of rules) {
    try {
      const regex = typeof rule.pattern === 'string' ? new RegExp(rule.pattern, 'gi') : rule.pattern;
      sanitized = sanitized.replace(regex, (match, offset) => {
        matches.push({
          ruleId: rule.id,
          originalText: match,
          replacementText: rule.replacement,
          startIndex: offset,
          endIndex: offset + match.length
        });
        return rule.replacement;
      });
    } catch (error) {
      emitTranscriptionError(sessionId, {
        type: 'transcription-error',
        code: TranscriptionErrorCode.RedactionRuleInvalid,
        message: `Invalid redaction rule: ${rule.id}`,
        recoverable: true,
        timestamp: new Date().toISOString(),
        context: { ruleId: rule.id, error }
      });
    }
  }

  return { content: sanitized, matches };
}
```

### Edge Case: Rapid Turn-Taking

```typescript
vadBus.on('server-vad', (signal) => {
  if (signal.state === 'stop') {
    aggregator.finalizePendingUtterance(signal.offset_ms);
    statusBus.emit({ type: 'transcription-status', status: 'thinking', sessionId, timestamp: new Date().toISOString() });
  }
});

voiceControlPanel.onUserInterrupt(() => {
  aggregator.cancelAssistantUtterances();
  statusBus.emit({ type: 'transcription-status', status: 'listening', sessionId, timestamp: new Date().toISOString(), detail: 'User interrupt' });
});
```

## 10. Validation Criteria

- STT service establishes transcription streaming within 1 second of session activation.
- Transcript delta and final events include required metadata, confidence scores (when supplied), and correct timestamps.
- Debounced updates meet latency and UI smoothness requirements without flooding messaging channels.
- Redaction/profanity rules operate prior to UI or persistence, with audit metadata stored transiently.
- Pausing and resuming during key renewals does not lose buffered audio or transcript context.
- Reconnection handling replays cached transcripts without duplication or ordering issues.
- Error events categorize faults correctly with actionable remediation guidance.
- Transcript history obeys memory and privacy constraints (size limits, non-persistence across sessions).
- Status events align with UI design states and remain localized.
- Logging includes correlation IDs and omits sensitive content.
- `session.update` requests set `input_audio_transcription` and `turn_detection` values that Azure acknowledges via `session.updated` events.
- `input_audio_buffer.speech_*` and `conversation.item.audio_transcription.*` events are surfaced to consumers and drive appropriate status transitions.
- Web Audio API 1.1 contracts hold: a singleton `AudioContext`, render quanta of 128 frames, and AudioWorklet-based processing (no `ScriptProcessorNode`).

## 11. Related Specifications / Further Reading

- [SP-005: Session Management & Renewal](sp-005-spec-design-session-management.md)
- [SP-006: WebRTC Audio Transport Layer](sp-006-spec-architecture-webrtc-audio.md)
- [SP-007: Audio Capture Pipeline Architecture](sp-007-spec-architecture-audio-capture-pipeline.md)
- [SP-001: Core Extension Activation & Lifecycle](sp-001-spec-architecture-extension-lifecycle.md)
- [SP-002: Configuration & Settings Management](sp-002-spec-design-configuration-management.md)
- [SP-027: Privacy & Data Handling Policy](sp-027-spec-security-privacy-data-handling.md) *(future dependency)*
- [docs/design/UI.md](../docs/design/UI.md)
- [docs/design/COMPONENTS.md](../docs/design/COMPONENTS.md)
- [Azure OpenAI Realtime Audio Quickstart](https://learn.microsoft.com/azure/ai-foundry/openai/realtime-audio-quickstart)
- [Azure OpenAI Realtime Event Schema](https://learn.microsoft.com/azure/ai-foundry/openai/realtime-reference)
- [Web Audio API 1.1 Specification](https://webaudio.github.io/web-audio-api/)
